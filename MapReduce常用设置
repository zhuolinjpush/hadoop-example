============设置输出snappy压缩============
String itime = args[0];
conf.set("itime", itime);
conf.setBoolean("mapreduce.output.fileoutputformat.compress", true);
conf.setClass("mapreduce.output.fileoutputformat.compress.codec", SnappyCodec.class, CompressionCodec.class);
conf.set("mapreduce.output.basename", itime);  //修改输出前缀
Job job = Job.getInstance(conf, "kpi-activearea-filling-" + itime);
job.setJarByClass(KpiActiveAreaFillingDataMR.class);
job.setMapperClass(StatMapper.class);
job.setReducerClass(StatReducer.class);
job.setMapOutputKeyClass(Text.class);
job.setMapOutputValueClass(Text.class);
job.setOutputKeyClass(NullWritable.class);
job.setOutputValueClass(Text.class);
job.setInputFormatClass(TextInputFormat.class);
job.setOutputFormatClass(TextOutputFormat.class);
job.setNumReduceTasks(50);
============= Reduce 输出多目录(run方法不用改) ===========
public static class StatReducer extends Reducer<LongWritable, Text, NullWritable, Text> {
        private static SimpleDateFormat sdf = new SimpleDateFormat("yyyyMMdd");
        private MultipleOutputs<NullWritable, Text> outputs;  
        @Override
        protected void setup(Context context) throws IOException,
                InterruptedException {
            outputs = new MultipleOutputs<NullWritable, Text>(context);
        }
        @Override
        protected void reduce(LongWritable key, Iterable<Text> values,
                Context context) throws IOException, InterruptedException {
            try {
                String[] arr = null;
                for (Text tx : values) {
                    arr = tx.toString().split("\t");
                    long itime = Long.parseLong((arr[5] + "000000000").substring(0, 13));
                    if (itime > 1481990400000L) {
                        outputs.write(NullWritable.get(), tx, "20161217/");
                    } else if (itime < 1451577600000L) {
                        outputs.write(NullWritable.get(), tx, "20160101/");
                    } else {
                        String iday = sdf.format(new Date(itime));
                        outputs.write(NullWritable.get(), tx, iday + "/");
                    }
                    
                }
            } catch (Exception e) {
                logger.error("reduce error", e);
            }
        }
        @Override
        protected void cleanup(Context context)
                throws IOException, InterruptedException {
            outputs.close();
        }
    }
    ------------ java read hdfs ----------
    	
java.io.IOException: No FileSystem for scheme: hdfs 添加如下：
config.set("fs.hdfs.impl",org.apache.hadoop.hdfs.DistributedFileSystem.class.getName());  
